{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt' \n",
    "# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('GloVe Loaded.')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "# Pre-trained GloVe embedding\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "word_vec_dim = len(embd[0]) # word_vec_dim = dimension of each word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk as nlp\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "summaries = []\n",
    "texts = []\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    return filter(lambda x: x in printable, text) #filter funny characters, if any. \n",
    "    \n",
    "\n",
    "with open('Reviews.csv', 'rb') as csvfile: #Data from https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "    Reviews = csv.DictReader(csvfile)\n",
    "    for row in Reviews:\n",
    "        clean_text = clean(row['Text'])\n",
    "        clean_summary = clean(row['Summary'])\n",
    "        summaries.append(word_tokenize(clean_summary))\n",
    "        texts.append(word_tokenize(clean_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE CLEANED & TOKENIZED TEXT: \n",
      "\n",
      "['my', 'kitty', 'is', '14', 'years', 'old', 'and', 'she', 'started', 'throwing', 'up', 'alot', 'this', 'spring', '.', 'i', 'switched', 'to', 'natural', 'balance', 'and', 'it', 'was', 'good', 'for', 'a', 'month', 'and', 'then', 'she', 'started', 'again', '.', 'so', 'i', 'tried', 'the', 'longevity', 'formula', 'and', 'so', 'far', 'have', 'had', 'great', 'results', '.', 'first', 'of', 'all', ',', 'she', 'seemed', 'to', 'like', 'the', 'taste', 'and', 'the', 'transition', 'to', 'a', 'new', 'food', 'was', 'a', 'breeze', '.', 'she', 'has', 'been', 'on', 'this', 'formula', 'for', 'a', 'couple', 'of', 'months', 'and', 'has', 'totally', 'quit', 'vomiting', '(', 'except', 'when', 'she', 'eats', 'grass', 'of', 'course', ')', ';', 'she', 'had', 'lost', 'some', 'weight', 'and', 'seems', 'more', 'fit', 'and', 'trim', '.', 'i', 'am', 'sooo', 'glad', 'and', 'highly', 'recommend', 'this', 'formula', '.', 'i', 'will', 'see', 'how', 'she', 'does', 'over', 'the', 'winter', 'when', 'she', 'is', 'less', 'active', ',', 'but', 'for', 'now', 'i', 'am', 'greatly', 'pleased', 'and', 'so', 'is', 'she', '!']\n",
      "\n",
      "SAMPLE CLEANED & TOKENIZED SUMMARY: \n",
      "\n",
      "['yes', '!', 'best', 'cat', 'food', '...', 'no', 'more', 'vomiting']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0,len(texts)-1)\n",
    "\n",
    "print \"SAMPLE CLEANED & TOKENIZED TEXT: \\n\\n\"+str(texts[index])\n",
    "print \"\\nSAMPLE CLEANED & TOKENIZED SUMMARY: \\n\\n\"+str(summaries[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_nearest_neighbour(x):\n",
    "    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n",
    "        \n",
    "    xdoty = np.multiply(embedding,x)\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embedding)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "\n",
    "    return embedding[np.argmax(cosine_similarities)]\n",
    "    \n",
    "\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('unk')]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in xrange(0, len(embedding)):\n",
    "            if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "                return vocab[x]\n",
    "    return vec2word(np_nearest_neighbour(np.asarray(vec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'unk':\n",
      "\n",
      "[ -7.91490018e-01   8.66169989e-01   1.19980000e-01   9.22870007e-04\n",
      "   2.77599990e-01  -4.91849989e-01   5.01950026e-01   6.07919996e-04\n",
      "  -2.58450001e-01   1.78650007e-01   2.53500015e-01   7.65720010e-01\n",
      "   5.06640017e-01   4.02500004e-01  -2.13879999e-03  -2.83969998e-01\n",
      "  -5.03239989e-01   3.04490000e-01   5.17790020e-01   1.50899999e-02\n",
      "  -3.50309998e-01  -1.12779999e+00   3.32529992e-01  -3.52499992e-01\n",
      "   4.13260013e-02   1.08630002e+00   3.39099988e-02   3.35640013e-01\n",
      "   4.97449994e-01  -7.01309964e-02  -1.21920002e+00  -4.85119998e-01\n",
      "  -3.85119990e-02  -1.35539994e-01  -1.63800001e-01   5.23209989e-01\n",
      "  -3.13180000e-01  -1.65500000e-01   1.19089998e-01  -1.51150003e-01\n",
      "  -1.56210005e-01  -6.26550019e-01  -6.23359978e-01  -4.21499997e-01\n",
      "   4.18729991e-01  -9.24719989e-01   1.10490000e+00  -2.99959987e-01\n",
      "  -6.30029989e-03   3.95399988e-01]\n"
     ]
    }
   ],
   "source": [
    "word = \"unk\"\n",
    "print \"Vector representation of '\"+str(word)+\"':\\n\"\n",
    "print word2vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REDUCE DATA (FOR SPEEDING UP THE NEXT STEPS)\n",
    "\n",
    "MAXIMUM_DATA_NUM = 50000\n",
    "\n",
    "texts = texts[0:MAXIMUM_DATA_NUM]\n",
    "summaries = summaries[0:MAXIMUM_DATA_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_limit = []\n",
    "embd_limit = []\n",
    "\n",
    "i=0\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)\n",
    "                embd_limit.append(word2vec(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for summary in summaries:\n",
    "    for word in summary:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)\n",
    "                embd_limit.append(word2vec(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'eos' not in vocab_limit:\n",
    "    vocab_limit.append('eos')\n",
    "    embd_limit.append(word2vec('eos'))\n",
    "if 'unk' not in vocab_limit:\n",
    "    vocab_limit.append('unk')\n",
    "    embd_limit.append(word2vec('unk'))\n",
    "\n",
    "null_vector = np.zeros([word_vec_dim])\n",
    "\n",
    "vocab_limit.append('<PAD>')\n",
    "embd_limit.append(null_vector)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_summaries = []\n",
    "\n",
    "for summary in summaries:\n",
    "    \n",
    "    vec_summary = []\n",
    "    \n",
    "    for word in summary:\n",
    "        vec_summary.append(word2vec(word))\n",
    "            \n",
    "    vec_summary.append(word2vec('eos'))\n",
    "    \n",
    "    vec_summary = np.asarray(vec_summary)\n",
    "    vec_summary = vec_summary.astype(np.float32)\n",
    "    \n",
    "    vec_summaries.append(vec_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_texts = []\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    vec_text = []\n",
    "    \n",
    "    for word in text:\n",
    "        vec_text.append(word2vec(word))\n",
    "    \n",
    "    vec_text = np.asarray(vec_text)\n",
    "    vec_text = vec_text.astype(np.float32)\n",
    "    \n",
    "    vec_texts.append(vec_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving processed data in another file.\n",
    "\n",
    "import pickle\n",
    "with open('vocab_limit', 'wb') as fp:\n",
    "    pickle.dump(vocab_limit, fp)\n",
    "with open('embd_limit', 'wb') as fp:\n",
    "    pickle.dump(embd_limit, fp)\n",
    "with open('vec_summaries', 'wb') as fp:\n",
    "    pickle.dump(vec_summaries, fp)\n",
    "with open('vec_texts', 'wb') as fp:\n",
    "    pickle.dump(vec_texts, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

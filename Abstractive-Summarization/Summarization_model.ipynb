{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization\n",
    "\n",
    "Loading pre-trained GloVe embeddings.\n",
    "Source of Data: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Another interesting embedding to look into:\n",
    "https://github.com/commonsense/conceptnet-numberbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "word_vec_dim = len(embedding[0])\n",
    "#Pre-trained GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will define functions for converting words to its vector representations and vice versa. \n",
    "\n",
    "### word2vec: \n",
    "\n",
    "Converts words to its vector representations.\n",
    "If the word is not present in the vocabulary, and thus if it doesn't have any vector representation,\n",
    "the word will be considered as 'unk' (denotes unknown) and the vector representation of unk will be\n",
    "returned instead. \n",
    "\n",
    "### np_nearest_neighbour:\n",
    "\n",
    "Returns the word vector in the vocabularity that is most similar\n",
    "to word vector given as an argument. The similarity is evaluated based on the formula of cosine\n",
    "similarity. \n",
    "\n",
    "### vec2word: \n",
    "\n",
    "Converts vectors to words. If the vector representation is unknown, and no corresponding word\n",
    "is known, then it returns the word representation of a known vector representation which is most similar \n",
    "to the vector given as argument (the np_nearest_neighbour() function is used for that).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_nearest_neighbour(x):\n",
    "    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n",
    "        \n",
    "    xdoty = np.multiply(embedding,x)\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embedding)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "\n",
    "    return embedding[np.argmax(cosine_similarities)]\n",
    "\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('unk')]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in xrange(0, len(embedding)):\n",
    "        if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "            return vocab[x]\n",
    "    return vec2word(np_nearest_neighbour(np.asarray(vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-processed dataset\n",
    "\n",
    "The Data is preprocessed in [Data_Pre-processing.ipynb](https://github.com/JRC1995/Abstractive-Summarization/blob/master/Data%20Pre-processing.ipynb)\n",
    "\n",
    "Dataset source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('vec_summaries', 'rb') as fp:\n",
    "    vec_summaries = pickle.load(fp)\n",
    "\n",
    "with open ('vec_texts', 'rb') as fp:\n",
    "    vec_texts = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am Loading vocab_limit and embd_limit (though I may not ever use embd_limit).\n",
    "Vocab_limit contains only vocabularies that are present in the dataset and \n",
    "some special words representing markers 'eos', '<PAD>' etc.\n",
    "\n",
    "The network should output the probability distribution over the words in \n",
    "vocab_limit. So using limited vocabulary (vocab_limit) will mean requiring\n",
    "less parameters for calculating the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('vocab_limit', 'rb') as fp:\n",
    "    vocab_limit = pickle.load(fp)\n",
    "\n",
    "with open ('embd_limit', 'rb') as fp:\n",
    "    embd_limit = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including SOS and its vector to the vocbularies. I forgot to do this while pre-processing.\n",
    "SOS signifies start of the decoder token input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_limit.append('<SOS>')\n",
    "embd_limit.append(np.zeros((word_vec_dim),dtype=np.float32))\n",
    "\n",
    "SOS = embd_limit[vocab_limit.index('<SOS>')]\n",
    "\n",
    "np_embd_limit = np.asarray(embd_limit,dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING DATA WITH SUMMARIES WHICH ARE TOO LONG\n",
    "\n",
    "I will not be training the model in batches. I will train the model one sample at a time, because my laptop\n",
    "will probably not be able to handle batch training (the kernel crashes now and then even with SGD ).\n",
    "\n",
    "However, if I was training in batches I had to choose a fixed maximum length for output.\n",
    "Each target output is marked with the word 'eos' at the end. After that each target output can be padded with\n",
    "'<PAD>' to fit the maximum output length. The network can be taught to produce an output in the form\n",
    "\"word1 word2 eos <PAD> <PAD>\". The batch training can be handled better if all target outputs are transformed\n",
    "to a fixed length. \n",
    "\n",
    "But, the fixed length should be less than or equal to the length of the longest target output so as to\n",
    "not discard any word from any target-output sample.\n",
    "\n",
    "But there may be a few very long target outputs\\summaries (say, 50+) whereas most summaries are near about\n",
    "length 10. So to fix the length, lots of padding has to be done to most of the summaries just because there\n",
    "are a few long summaries. \n",
    "\n",
    "Better to just remove the data whose summaries are bigger than a specified threshold (MAX_SUMMARY_LEN).\n",
    "In this cell I will diagnose how many percentage of data will be removed for a given threshold length,\n",
    "and in the next cell I will remove them.\n",
    "\n",
    "Note: I am comparing len(summary_vec)-1, instead of len(summary_vec). The reason is that I am ignoring \n",
    "the last word vector which is the representation of the 'eos' marker. I will explain why later on this\n",
    "notebook. \n",
    "\n",
    "### REMOVING DATA WITH TEXTS WHOSE LENGTH IS SMALLER THAN THE WINDOW SIZE\n",
    "\n",
    "In this model I will try to implement <b>local attention</b> with standard encoder-decoder architecture.\n",
    "\n",
    "Where global attention looks at all the hidden states of the encoder to determine where to attend to,\n",
    "local attention looks only at the hidden states under the range pt-D to pt+D where D is empirically selected\n",
    "and pt is a position determined by the program.\n",
    "The range of pt-D to pt+D can be said to be the window where attention takes place.  Pt is the center of the\n",
    "window.\n",
    "\n",
    "I am treating D as a hyperparameter. The window size will be (pt-D)-(pt+D)+1 = 2D+1.\n",
    "\n",
    "Now, obviously, the window needs to be smaller than or equal to the no. of the encoded hidden states themselves.\n",
    "We will encode one hidden state for each words in the input text, so size of the hidden states will be equivalent\n",
    "to the size of the input text.\n",
    "\n",
    "So we must choose D such that 2D+1 is not bigger than the length of any text in the dataset.\n",
    "\n",
    "To ensure that, I will first diagnose how many data will be removed for a given D, and in the next cell,\n",
    "I will remove all input texts whose length is less than 2D+1.\n",
    "\n",
    "### REMOVING DATA WITH TEXTS(REVIEWS) WHICH ARE TOO LONG\n",
    "\n",
    "The RNN encoders will encode one word at a time. No. of words in the text data or in other words,\n",
    "the length of the text size will also be the no. of timesteps for the encoder RNN. To make the training less intensive \n",
    "(so that it doesn't burden my laptop too much), I will be removing\n",
    "all data with whose review size exceeds a given threshold (MAX_TEXT_LEN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of dataset with summary length beyond 7: 16.146% \n",
      "Percentage of dataset with text length less that window size: 2.258% \n",
      "Percentage of dataset with text length more than 80: 40.412% \n"
     ]
    }
   ],
   "source": [
    "#DIAGNOSIS\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 7\n",
    "\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with summary length beyond \"+str(LEN)+\": \"+str((count/len(vec_summaries))*100)+\"% \"\n",
    "\n",
    "count = 0\n",
    "\n",
    "D = 10 \n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)<window_size+1:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with text length less that window size: \"+str((count/len(vec_texts))*100)+\"% \"\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 80\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with text length more than \"+str(LEN)+\": \"+str((count/len(vec_texts))*100)+\"% \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will start the aformentioned removal process.\n",
    "vec_summary_reduced and vec_texts_reduced will contain the remaining data after the removal.\n",
    "\n",
    "<b>Note: an important hyperparameter D is initialized here.</b>\n",
    "\n",
    "D determines the window size of local attention as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SUMMARY_LEN = 7\n",
    "MAX_TEXT_LEN = 80\n",
    "\n",
    "#D is a major hyperparameters. Windows size for local attention will be 2*D+1\n",
    "D = 10\n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "#REMOVE DATA WHOSE SUMMARIES ARE TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS SMALLED THAN WINDOW SIZE\n",
    "\n",
    "vec_summaries_reduced = []\n",
    "vec_texts_reduced = []\n",
    "\n",
    "i = 0\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1<=MAX_SUMMARY_LEN and len(vec_texts[i])>=window_size and len(vec_texts[i])<=MAX_TEXT_LEN:\n",
    "        vec_summaries_reduced.append(summary)\n",
    "        vec_texts_reduced.append(vec_texts[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will start the aformentioned removal process.\n",
    "vec_summary_reduced and vec_texts_reduced will contain the remaining data after the removal.\n",
    "\n",
    "<b>Note: an important hyperparameter D is initialized here.</b>\n",
    "\n",
    "D determines the window size of local attention as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int((.7)*len(vec_summaries_reduced))\n",
    "\n",
    "train_texts = vec_texts_reduced[0:train_len]\n",
    "train_summaries = vec_summaries_reduced[0:train_len]\n",
    "\n",
    "val_len = int((.15)*len(vec_summaries_reduced))\n",
    "\n",
    "val_texts = vec_texts_reduced[train_len:train_len+val_len]\n",
    "val_summaries = vec_summaries_reduced[train_len:train_len+val_len]\n",
    "\n",
    "test_texts = vec_texts_reduced[train_len+val_len:len(vec_summaries_reduced)]\n",
    "test_summaries = vec_summaries_reduced[train_len+val_len:len(vec_summaries_reduced)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18293\n"
     ]
    }
   ],
   "source": [
    "print train_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function transform_out() will convert the target output sample so that \n",
    "it can be in a format which can be used by tensorflow's \n",
    "sparse_softmax_cross_entropy_with_logits() for loss calculation.\n",
    "\n",
    "Think of one hot encoding. This transformation is kind of like that.\n",
    "All the words in the vocab_limit are like classes in this context.\n",
    "\n",
    "However, instead of being precisely one hot encoded the output will be transformed\n",
    "such that it will contain the list of indexes which would have been 'one' if it was one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_out(output_text):\n",
    "    output_len = len(output_text)\n",
    "    transformed_output = np.zeros([output_len],dtype=np.int32)\n",
    "    for i in xrange(0,output_len):\n",
    "        transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n",
    "    return transformed_output   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Here I am simply setting up some of the rest of the hyperparameters.\n",
    "K, here, is a special hyperparameter. It denotes the no. of previous hidden states\n",
    "to consider for residual connections. More on that later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some MORE hyperparameters and other stuffs\n",
    "\n",
    "hidden_size = 500\n",
    "learning_rate = 0.003\n",
    "K = 5\n",
    "vocab_len = len(vocab_limit)\n",
    "training_iters = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up tensorflow placeholders.\n",
    "The purpose of the placeholders are pretty much self explanatory from the name.\n",
    "\n",
    "Note: tf_seq_len, and tf_output_len aren't really necessary. They can be derived \n",
    "from tf_text and tf_summary respectively, but I ended up making them anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#placeholders\n",
    "tf_text = tf.placeholder(tf.float32, [None,word_vec_dim])\n",
    "tf_seq_len = tf.placeholder(tf.int32)\n",
    "tf_summary = tf.placeholder(tf.int32,[None])\n",
    "tf_output_len = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORWARD AND BACKWARD LSTM WITH RRA\n",
    "\n",
    "I will be using the encoder-decoder architecture.\n",
    "For the encoder I will be using a bi-directional LSTM.\n",
    "Below is the function of the forward encoder (the LSTM in the forward direction\n",
    "that starts from the first word and encodes a word in the context of previous words),\n",
    "and then for the backward encoder (the LSTM in the backward direction\n",
    "that starts from the last word and encodes a word in the context of later words)\n",
    "\n",
    "The RNN used here, is a standard LSTM with RRA ([Residual Recurrent Attention](https://arxiv.org/abs/1709.03714))\n",
    "\n",
    "Remember, the hyperparameter K?\n",
    "\n",
    "The model will compute the weighted sum (weighted based on some trainable parameters\n",
    "in the attention weight matrix) of the PREVIOUS K hidden states - the weighted sum\n",
    "is denoted as RRA in this function.\n",
    "\n",
    "hidden_residuals will contain the last K hidden states.\n",
    "\n",
    "The RRA will influence the Hidden State calculation in LSTM.\n",
    "\n",
    "(The attention weight matrix is to be normalized by dividing each elements by the sum of all \n",
    "the elements as said in the paper. But, here, I am normalizing it by softmax)\n",
    "\n",
    "The purpose for this is to created connections between hidden states of different timesteps,\n",
    "to establish long term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_encoder(inp,hidden,cell,\n",
    "                    wf,uf,bf,\n",
    "                    wi,ui,bi,\n",
    "                    wo,uo,bo,\n",
    "                    wc,uc,bc,\n",
    "                    Wattention,seq_len,inp_dim):\n",
    "\n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_forward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=0\n",
    "    j=K\n",
    "    \n",
    "    def cond(i,j,hidden,cell,hidden_forward,hidden_residuals):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,j,hidden,cell,hidden_forward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        # LSTM with RRA\n",
    "        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "        \n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "\n",
    "        hidden_forward = hidden_forward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i+1,j+1,hidden,cell,hidden_forward,hidden_residuals\n",
    "    \n",
    "    _,_,_,_,hidden_forward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_forward,hidden_residuals])\n",
    "    \n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_forward.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_encoder(inp,hidden,cell,\n",
    "                     wf,uf,bf,\n",
    "                     wi,ui,bi,\n",
    "                     wo,uo,bo,\n",
    "                     wc,uc,bc,\n",
    "                     Wattention,seq_len,inp_dim):\n",
    "    \n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_backward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=seq_len-1\n",
    "    j=K\n",
    "    \n",
    "    def cond(i,j,hidden,cell,hidden_backward,hidden_residuals):\n",
    "        return i > -1\n",
    "    \n",
    "    def body(i,j,hidden,cell,hidden_backward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        # LSTM with RRA\n",
    "        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "\n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "        \n",
    "        hidden_backward = hidden_backward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i-1,j+1,hidden,cell,hidden_backward,hidden_residuals\n",
    "    \n",
    "    _,_,_,_,hidden_backward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_backward,hidden_residuals])\n",
    "\n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_backward.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder similarly uses LSTM with RRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x,hidden,cell,\n",
    "            wf,uf,bf,\n",
    "            wi,ui,bi,\n",
    "            wo,uo,bo,\n",
    "            wc,uc,bc,RRA):\n",
    "    \n",
    "    # LSTM with RRA\n",
    "    fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "    ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "    og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "    cell_next = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "    hidden_next = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "    \n",
    "    return hidden_next,cell_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOCAL ATTENTION:\n",
    "\n",
    "The cell below includes some major functions for the attention mechanism.\n",
    "\n",
    "The attention mechanism is usually implemented to compute an attention score \n",
    "for each of the encoded hidden state in the context of a particular\n",
    "decoder hidden state in each timestep - all to determine which encoded hidden\n",
    "states to attend to for a particular decoder hidden state context.\n",
    "\n",
    "More specifically, I am here implementing local attention as opposed to global attention.\n",
    "\n",
    "I already mentioned local attention before. Local attention mechanism involves focusing on\n",
    "a subset of encoded hidden states, whereas a gloabl attention mechanism invovles focusing on all\n",
    "the encoded hidden states.\n",
    "\n",
    "This is the paper on which this implementation is based on:\n",
    "https://nlp.stanford.edu/pubs/emnlp15_attn.pdf\n",
    "    \n",
    "Following the formulas presented in the paper, first, I am computing\n",
    "the position pt (the center of the window of attention).\n",
    "\n",
    "pt is simply a position in the sequence.\n",
    "For a given pt, the model will only consider the hidden state starting from the position\n",
    "pt-D to the hidden state at the position pt+D. \n",
    "\n",
    "To say a hidden state is at position p, I mean to say that the hidden state is the encoded\n",
    "representation of a word at position p in the sequence.\n",
    "\n",
    "The paper formulates the equation for calculating pt like this:\n",
    "pt = sequence_length x sigmoid(..some linear algebras and activations...)\n",
    "\n",
    "But, I didn't used the sequence_length of the whole text which is tf_seq_len but 'positions' which\n",
    "is = tf_seq_len-1-2D\n",
    "\n",
    "if pt = tf_seq_len x sigmoid(tensor)\n",
    "\n",
    "Then pt will be in the range 0 to tf_seq_len\n",
    "\n",
    "But, we can't have that. There is no tf_seq_len position. Since the length is tf_seq_len,\n",
    "the available positions are 0 to (tf_seq_len-1). Which is why I subtracted 1 from it.\n",
    "\n",
    "Next, we must have the value of pt to be such that it represents the CENTER of the window.\n",
    "If pt is too close to 0, pt-D will be negative - a non-existent position.\n",
    "If pt is too close to tf_seq_len, pt+D will be a non-existent position.\n",
    "\n",
    "So pt can't occupy the first D positions (0 to D-1) and it can't occupy the last D positions\n",
    "((tf_seq_len-D) to (tf_seq_len-1)) in order to keep pt-D and pt+D as legal positions.\n",
    "So a total 2D positions should be restricted to pt.\n",
    "\n",
    "Which is why I further subtracted 2D from tf_seq_len.\n",
    "\n",
    "Still, after calculating pt = positions x sigmoid(tensor)\n",
    "where positions = tf_seq_len-(2D+1), \n",
    "pt will merely range between 0 to tf_seq_len-(2D+1)\n",
    "\n",
    "We can't still accept pt to be 0 since pt-D will be negative. But the length of the range \n",
    "of integer positions pt can occupy is now perfect.\n",
    "\n",
    "So at this point, we can simply center pt at the window by adding a D.\n",
    "\n",
    "After that, pt will range from D to (tf_seq_len-1)-D\n",
    "\n",
    "Now, it can be checked that pt+D, or pt-D will never become negative or exceed\n",
    "the total sequence length.\n",
    "\n",
    "After calculating pt, we can use the formulas presented in the paper to calculate\n",
    "the G score which signifies the weight (or attention) that should be given to a hidden state.\n",
    "\n",
    "G scores is calculated for each of hidden states in the local window. This is equivalent to\n",
    "a(s) used in the paper.\n",
    "\n",
    "The function returns the G scores and the position pt, so that the model can create the \n",
    "context vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(hs,ht,Wa,seq_len):\n",
    "    return tf.reshape(tf.matmul(tf.matmul(hs,Wa),tf.transpose(ht)),[seq_len])\n",
    "\n",
    "def align(hs,ht,Wp,Vp,Wa,tf_seq_len):\n",
    "   \n",
    "    pd = tf.TensorArray(size=(2*D+1),dtype=tf.float32)\n",
    "    \n",
    "    positions = tf.cast(tf_seq_len-1-2*D,dtype=tf.float32)\n",
    "    \n",
    "    sigmoid_multiplier = tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(ht,Wp)),Vp))\n",
    "    sigmoid_multiplier = tf.reshape(sigmoid_multiplier,[])\n",
    "    \n",
    "    pt_float = positions*sigmoid_multiplier\n",
    "    \n",
    "    pt = tf.cast(pt_float,tf.int32)\n",
    "    pt = pt+D #center to window\n",
    "    \n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    i = 0\n",
    "    pos = pt - D\n",
    "    \n",
    "    def cond(i,pos,pd):\n",
    "        \n",
    "        return i < (2*D+1)\n",
    "                      \n",
    "    def body(i,pos,pd):\n",
    "        \n",
    "        comp_1 = tf.cast(tf.square(pos-pt),tf.float32)\n",
    "        comp_2 = tf.cast(2*tf.square(sigma),tf.float32)\n",
    "            \n",
    "        pd = pd.write(i,tf.exp(-(comp_1/comp_2)))\n",
    "            \n",
    "        return i+1,pos+1,pd\n",
    "                      \n",
    "    i,pos,pd = tf.while_loop(cond,body,[i,pos,pd])\n",
    "    \n",
    "    local_hs = hs[(pt-D):(pt+D+1)]\n",
    "    \n",
    "    normalized_scores = tf.nn.softmax(score(local_hs,ht,Wa,2*D+1))\n",
    "    \n",
    "    pd=pd.stack()\n",
    "    \n",
    "    G = tf.multiply(normalized_scores,pd)\n",
    "    G = tf.reshape(G,[2*D+1,1])\n",
    "    \n",
    "    return G,pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL DEFINITION\n",
    "\n",
    "First is the <b>bi-directional encoder</b>.\n",
    "\n",
    "h_forward is the tensorarray of all the hidden states from the \n",
    "forward encoder whereas h_backward is the tensorarray of all the hidden states\n",
    "from the backward encoder.\n",
    "\n",
    "The final list of encoder hidden states are usually calculated by combining \n",
    "the equivalents of h_forward and h_backward by some means.\n",
    "\n",
    "There are many means of combining them, like: concatenation, summation, average etc.\n",
    "    \n",
    "I will be using concatenation.\n",
    "\n",
    "hidden_encoder is the final list of encoded hidden state\n",
    "\n",
    "The first decoder input is the word vector representation of <SOS> which siginfies the start of decoding.\n",
    "\n",
    "I am using the first encoded_hidden_state \n",
    "as the initial decoder state. The first encoded_hidden_state may have the least \n",
    "past context (none actually) but, it will have the most future context.\n",
    "\n",
    "The next decoder hidden state is generated from the initial decoder input and the initial decoder state.\n",
    "Next, I start a loop which iterates for output_len times. \n",
    "\n",
    "Next the <b>attention function</b> is called, to compute the G score by scoring the encoder hidden states\n",
    "in term of current decoder hidden step.\n",
    "\n",
    "The context vector is created by the weighted (weighted in terms of G scores) summation\n",
    "of hidden states in the local attention window.\n",
    "\n",
    "I used the formulas mentioned here: https://nlp.stanford.edu/pubs/emnlp15_attn.pdf\n",
    "\n",
    "to calculate the probability distribution for the first output token from the context vector and decoder hidden state.\n",
    "\n",
    "The word vector represention of the output token - the word with maximum the predicted probability in the recently calculated probability distribution, is used as the decoder input token. The output decoder hidden state from that current decoder input token, and the hidden state, is used again in the next loop to calculate the probability distribution of the next output token and so on. \n",
    "\n",
    "('beam search' is another approach to look into at this part)\n",
    "\n",
    "The loop continues for 'output_len' no. of iterations. \n",
    "\n",
    "Since I will be training sample to sample, I can dynamically send the output length \n",
    "of the current sample, and the decoder loops for the given 'output length' times.\n",
    "\n",
    "NOTE: I am saving only the (non-softmaxed) probability distributions for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(tf_text,tf_seq_len,tf_output_len):\n",
    "    \n",
    "    #PARAMETERS\n",
    "    \n",
    "    #1.1 FORWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    cell_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    wf_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uf_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bf_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wi_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    ui_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bi_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wo_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uo_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bo_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wc_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uc_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bc_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_f = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "                               \n",
    "    #1.2 BACKWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    cell_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    wf_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uf_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bf_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wi_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    ui_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bi_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wo_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uo_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bo_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wc_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uc_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bc_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_b = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    #2 ATTENTION PARAMETERS\n",
    "    \n",
    "    Wp = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,50],stddev=0.01))\n",
    "    Vp = tf.Variable(tf.truncated_normal(shape=[50,1],stddev=0.01))\n",
    "    Wa = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    Wc = tf.Variable(tf.truncated_normal(shape=[4*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    \n",
    "    #3 DECODER PARAMETERS\n",
    "    \n",
    "    Ws = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,vocab_len],stddev=0.01))\n",
    "    \n",
    "    cell_d = tf.zeros([1,2*hidden_size],dtype=tf.float32)\n",
    "    wf_d = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,2*hidden_size],stddev=0.01))\n",
    "    uf_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bf_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wi_d = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,2*hidden_size],stddev=0.01))\n",
    "    ui_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bi_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wo_d = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,2*hidden_size],stddev=0.01))\n",
    "    uo_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bo_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wc_d = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,2*hidden_size],stddev=0.01))\n",
    "    uc_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bc_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals_d = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals_d = hidden_residuals_d.unstack(tf.zeros([K,2*hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    Wattention_d = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    output = tf.TensorArray(size=tf_output_len,dtype=tf.float32)\n",
    "                               \n",
    "    #BI-DIRECTIONAL LSTM\n",
    "                               \n",
    "    hidden_forward = forward_encoder(tf_text,\n",
    "                                     initial_hidden_f,cell_f,\n",
    "                                     wf_f,uf_f,bf_f,\n",
    "                                     wi_f,ui_f,bi_f,\n",
    "                                     wo_f,uo_f,bo_f,\n",
    "                                     wc_f,uc_f,bc_f,\n",
    "                                     Wattention_f,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    hidden_backward = backward_encoder(tf_text,\n",
    "                                     initial_hidden_b,cell_b,\n",
    "                                     wf_b,uf_b,bf_b,\n",
    "                                     wi_b,ui_b,bi_b,\n",
    "                                     wo_b,uo_b,bo_b,\n",
    "                                     wc_b,uc_b,bc_b,\n",
    "                                     Wattention_b,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    encoded_hidden = tf.concat([hidden_forward,hidden_backward],1)\n",
    "    \n",
    "    #ATTENTION MECHANISM AND DECODER\n",
    "    \n",
    "    decoded_hidden = encoded_hidden[0]\n",
    "    decoded_hidden = tf.reshape(decoded_hidden,[1,2*hidden_size])\n",
    "    Wattention_d_normalized = tf.nn.softmax(Wattention_d)\n",
    "    tf_embd_limit = tf.convert_to_tensor(np_embd_limit)\n",
    "    \n",
    "    y = tf.convert_to_tensor(SOS) #inital decoder token <SOS> vector\n",
    "    y = tf.reshape(y,[1,word_vec_dim])\n",
    "    \n",
    "    j=K\n",
    "    \n",
    "    hidden_residuals_stack = hidden_residuals_d.stack()\n",
    "    \n",
    "    RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n",
    "    RRA = tf.reshape(RRA,[1,2*hidden_size])\n",
    "    \n",
    "    decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n",
    "                                  wf_d,uf_d,bf_d,\n",
    "                                  wi_d,ui_d,bf_d,\n",
    "                                  wo_d,uo_d,bf_d,\n",
    "                                  wc_d,uc_d,bc_d,\n",
    "                                  RRA)\n",
    "    decoded_hidden = decoded_hidden_next\n",
    "    \n",
    "    hidden_residuals_d = hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size]))\n",
    "    \n",
    "    j=j+1\n",
    "                           \n",
    "    i=0\n",
    "    \n",
    "    def attention_decoder_cond(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n",
    "        return i < tf_output_len\n",
    "    \n",
    "    def attention_decoder_body(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n",
    "        \n",
    "        #LOCAL ATTENTION\n",
    "        \n",
    "        G,pt = align(encoded_hidden,decoded_hidden,Wp,Vp,Wa,tf_seq_len)\n",
    "        local_encoded_hidden = encoded_hidden[pt-D:pt+D+1]\n",
    "        weighted_encoded_hidden = tf.multiply(local_encoded_hidden,G)\n",
    "        context_vector = tf.reduce_sum(weighted_encoded_hidden,0)\n",
    "        context_vector = tf.reshape(context_vector,[1,2*hidden_size])\n",
    "        \n",
    "        attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,decoded_hidden],1),Wc))\n",
    "        \n",
    "        #DECODER\n",
    "        \n",
    "        y = tf.matmul(attended_hidden,Ws)\n",
    "        \n",
    "        output = output.write(i,tf.reshape(y,[vocab_len]))\n",
    "        #Save probability distribution as output\n",
    "        \n",
    "        y = tf.nn.softmax(y)\n",
    "        \n",
    "        y_index = tf.cast(tf.argmax(tf.reshape(y,[vocab_len])),tf.int32)\n",
    "        y = tf_embd_limit[y_index]\n",
    "        y = tf.reshape(y,[1,word_vec_dim])\n",
    "        \n",
    "        #setting next decoder input token as the word_vector of maximum probability \n",
    "        #as found from previous attention-decoder output.\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals_d.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n",
    "        RRA = tf.reshape(RRA,[1,2*hidden_size])\n",
    "        \n",
    "        decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n",
    "                                  wf_d,uf_d,bf_d,\n",
    "                                  wi_d,ui_d,bf_d,\n",
    "                                  wo_d,uo_d,bf_d,\n",
    "                                  wc_d,uc_d,bc_d,\n",
    "                                  RRA)\n",
    "        \n",
    "        decoded_hidden = decoded_hidden_next\n",
    "        \n",
    "        hidden_residuals_d = tf.cond(tf.equal(j,tf_output_len-1+K+1), #(+1 for <SOS>)\n",
    "                                   lambda: hidden_residuals_d,\n",
    "                                   lambda: hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size])))\n",
    "        \n",
    "        return i+1,j+1,decoded_hidden,cell_d,hidden_residuals_d,output\n",
    "    \n",
    "    i,j,decoded_hidden,cell_d,hidden_residuals_d,output = tf.while_loop(attention_decoder_cond,\n",
    "                                            attention_decoder_body,\n",
    "                                            [i,j,decoded_hidden,cell_d,hidden_residuals_d,output])\n",
    "    hidden_residuals_d.close().mark_used()\n",
    "    \n",
    "    output = output.stack()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model function is initiated here. The output is\n",
    "computed. Cost function and optimizer are defined.\n",
    "I am creating a prediction tensorarray which will \n",
    "store the index of maximum element of \n",
    "the output probability distributions.\n",
    "From that index I can find the word in vocab_limit\n",
    "which is represented by it. So the final visible\n",
    "predictions will be the words that the model decides to\n",
    "be most probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tf_text,tf_seq_len,tf_output_len)\n",
    "\n",
    "#OPTIMIZER\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=tf_summary))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#PREDICTION\n",
    "\n",
    "pred = tf.TensorArray(size=tf_output_len,dtype=tf.int32)\n",
    "\n",
    "i=0\n",
    "\n",
    "def cond_pred(i,pred):\n",
    "    return i<tf_output_len\n",
    "def body_pred(i,pred):\n",
    "    pred = pred.write(i,tf.cast(tf.argmax(output[i]),tf.int32))\n",
    "    return i+1,pred\n",
    "\n",
    "i,pred = tf.while_loop(cond_pred,body_pred,[i,pred]) \n",
    "\n",
    "prediction = pred.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING\n",
    "\n",
    "Finally, this is where training takes place.\n",
    "It's all pretty self explanatory, but one thing to note is that\n",
    "I am sending \"train_summaries[i][0:len(train_summaries[i])-1]\"\n",
    "to the transform_out() function. That is, I am ignoring the last\n",
    "word from summary. The last word marks the end of the summary.\n",
    "It's 'eos'. \n",
    "\n",
    "I trained it before without dynamically feeding the output_len.\n",
    "Ideally the network should determine the output_len by itself.\n",
    "\n",
    "Which is why I defined (in past) a MAX_LEN, and transformed target outputs in\n",
    "the form \"word1 word2 word3....eos <PAD> <PAD>....until max_length\"\n",
    "I created the model output in the same way.\n",
    "\n",
    "The model would ideally learn in which context and where to put eos.\n",
    "And then the only the portion before eos can be shown to the user.\n",
    "\n",
    "After training, the model can even be modified to run until,\n",
    "the previous output y denotes an eos. \n",
    "\n",
    "That way, we can have variable length output, with the length decided\n",
    "by the model itself, not the user.\n",
    "\n",
    "But all the padding and eos, makes the model to come in contact with \n",
    "pads and eos in most of the target output. The model learns to consider eos and \n",
    "pad to be important. Trying to fit to the data, the early model starts to\n",
    "spam eos and pad in its predicted output.\n",
    "\n",
    "That necessarily isn't a problem. The model may learn to fare better\n",
    "later on, but I planned only to check a couple of early iterations, \n",
    "and looking at predictions consisting of only eos and pads\n",
    "isn't too interesting. I wanted to check what kind of words (other than\n",
    "eos and pads) the model learns to produce in the early iterations. \n",
    "\n",
    "Which is why I am doing what I am doing. Ideally, my past implemention\n",
    "waould be better. \n",
    "\n",
    "As I said before, I will run it for only a few early iterations.\n",
    "So, it's not likely to see any great predicted summaries here.\n",
    "As can be seen, the summaries seem more influenced by previous \n",
    "output sample than the input context in these early iterations.\n",
    "\n",
    "Some of the texts contains undesirable words like br tags and so\n",
    "on. So better preprocessing and tokenization may be desirable.\n",
    "\n",
    "With more layer depth, larger hidden size, mini-batch training,\n",
    "and other changes, this model may have potential, or may not.\n",
    "\n",
    "The same arcitechture should be usable for training on translation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 0\n",
      "Training input sequence length: 51\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "i have bought several of the vitality canned dog food products and have found them all to be of good quality. the product looks more like a stew than a processed meat and it smells better. my labrador is finicky and she appreciates this product better than most.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "pricey pricey substantive substantive\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good quality dog food\n",
      "\n",
      "loss=10.4001\n",
      "\n",
      "Iteration: 1\n",
      "Training input sequence length: 37\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "product arrived labeled as jumbo salted peanuts ... the peanuts were actually small sized unsalted. not sure if this was an error or if the vendor intended to represent the product as `` jumbo ''.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "dog dog food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "not as advertised\n",
      "\n",
      "loss=10.4679\n",
      "\n",
      "Iteration: 2\n",
      "Training input sequence length: 46\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "if you are looking for the secret ingredient in robitussin i believe i have found it. i got this in addition to the root beer extract i ordered( which was good) and made some cherry soda. the flavor is very medicinal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "dog dog\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "cough medicine\n",
      "\n",
      "loss=10.342\n",
      "\n",
      "Iteration: 3\n",
      "Training input sequence length: 32\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "great taffy at a great price. there was a wide assortment of yummy taffy. delivery was very quick. if your a taffy lover, this is a deal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "as as\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great taffy\n",
      "\n",
      "loss=10.7917\n",
      "\n",
      "Iteration: 4\n",
      "Training input sequence length: 30\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this taffy is so good. it is very soft and chewy. the flavors are amazing. i would definitely recommend you buying it. very satisfying!!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "as advertised advertised advertised\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wonderful, tasty taffy\n",
      "\n",
      "loss=10.9395\n",
      "\n",
      "Iteration: 5\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "right now i 'm mostly just sprouting this so my cats can eat the grass. they love it. i rotate it around with wheatgrass and rye too\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not advertised\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "yay barley\n",
      "\n",
      "loss=13.1798\n",
      "\n",
      "Iteration: 6\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "this is a very healthy dog food. good for their digestion. also good for small puppies. my dog eats her required amount at every feeding.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "cough medicine medicine\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "healthy dog food\n",
      "\n",
      "loss=8.07541\n",
      "\n",
      "Iteration: 7\n",
      "Training input sequence length: 24\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "the strawberry twizzlers are my guilty pleasure- yummy. six pounds will be around for a while with my son and i.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "cough medicine medicine medicine\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "strawberry twizzlers- yummy\n",
      "\n",
      "loss=14.3803\n",
      "\n",
      "Iteration: 8\n",
      "Training input sequence length: 45\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i love eating them and they are good for watching tv and looking at movies! it is not too sweet. i like to transfer them to a zip lock baggie so they stay fresh so i can take my time eating them.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "cough taffy\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "poor taste\n",
      "\n",
      "loss=14.6777\n",
      "\n",
      "Iteration: 9\n",
      "Training input sequence length: 28\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i am very satisfied with my unk purchase. i shared these with others and we have all enjoyed them. i will definitely be ordering more.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "taffy taffy food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "love it!\n",
      "\n",
      "loss=14.7676\n",
      "\n",
      "Iteration: 10\n",
      "Training input sequence length: 31\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "candy was delivered very fast and was purchased at a reasonable price. i was home bound and unable to get to a store so this was perfect for me.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great dog food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "home delivered unk\n",
      "\n",
      "loss=14.3403\n",
      "\n",
      "Iteration: 11\n",
      "Training input sequence length: 52\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "my husband is a twizzlers addict. we 've bought these many times from amazon because we 're government employees living overseas and ca n't get them in the country we are assigned to. they 've always been fresh and tasty, packed well and arrive in a timely manner.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "always fresh\n",
      "\n",
      "loss=12.2007\n",
      "\n",
      "Iteration: 12\n",
      "Training input sequence length: 68\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i bought these for my husband who is currently overseas. he loves these, and apparently his staff likes them unk< br/> there are generous amounts of twizzlers in each 16-ounce bag, and this was well worth the price.< a unk '' http: unk ''> twizzlers, strawberry, 16-ounce bags( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "loss=5.20999\n",
      "\n",
      "Iteration: 13\n",
      "Training input sequence length: 31\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i can remember buying this candy as a kid and the quality has n't dropped in all these years. still a superb product you wo n't be disappointed with.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "yay barley food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "delicious product!\n",
      "\n",
      "loss=9.56776\n",
      "\n",
      "Iteration: 14\n",
      "Training input sequence length: 21\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i love this candy. after weight watchers i had to cut back but still have a craving for it.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "loss=1.86481\n",
      "\n",
      "Iteration: 15\n",
      "Training input sequence length: 72\n",
      "Training target outputs sequence length: 7\n",
      "\n",
      "TEXT:\n",
      "i have lived out of the us for over 7 yrs now, and i so miss my twizzlers!! when i go back to visit or someone visits me, i always stock up. all i can say is yum!< br/> sell these in mexico and you will have a faithful buyer, more often than i 'm able to buy them right now.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers twizzlers twizzlers twizzlers twizzlers twizzlers twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "please sell these in mexico!!\n",
      "\n",
      "loss=10.0658\n",
      "\n",
      "Iteration: 16\n",
      "Training input sequence length: 36\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "product received is as unk< br/>< br/>< a unk '' http: unk ''> twizzlers, strawberry, 16-ounce bags( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers twizzlers twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers- strawberry\n",
      "\n",
      "loss=3.0101\n",
      "\n",
      "Iteration: 17\n",
      "Training input sequence length: 43\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "i was so glad amazon carried these batteries. i have a hard time finding them elsewhere because they are such a unique size. i need them for my garage door unk< br/> great deal for the price.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers twizzlers!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great bargain for the price\n",
      "\n",
      "loss=13.083\n",
      "\n",
      "Iteration: 18\n",
      "Training input sequence length: 26\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "this offer is a great price and a great taste, thanks amazon for selling this unk< br/>< br/> unk\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers twizzlers!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "this is my taste ...\n",
      "\n",
      "loss=12.1877\n",
      "\n",
      "Iteration: 19\n",
      "Training input sequence length: 60\n",
      "Training target outputs sequence length: 7\n",
      "\n",
      "TEXT:\n",
      "for those of us with celiac disease this product is a lifesaver and what could be better than getting it at almost half the price of the grocery or health food store! i love mccann 's instant oatmeal- all"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from __future__ import print_function\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 0   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    display_step = 1\n",
    "    \n",
    "    while step < training_iters:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "           \n",
    "        for i in xrange(0,train_len):\n",
    "            \n",
    "            train_out = transform_out(train_summaries[i][0:len(train_summaries[i])-1])\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nIteration: \"+str(i))\n",
    "                print(\"Training input sequence length: \"+str(len(train_texts[i])))\n",
    "                print(\"Training target outputs sequence length: \"+str(len(train_out)))\n",
    "            \n",
    "                print(\"\\nTEXT:\")\n",
    "                flag = 0\n",
    "                for vec in train_texts[i]:\n",
    "                    if vec2word(vec) in string.punctuation or flag==0:\n",
    "                        print(str(vec2word(vec)),end='')\n",
    "                    else:\n",
    "                        print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "\n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,pred = sess.run([optimizer,cost,prediction],feed_dict={tf_text: train_texts[i], \n",
    "                                                    tf_seq_len: len(train_texts[i]), \n",
    "                                                    tf_summary: train_out,\n",
    "                                                    tf_output_len: len(train_out)})\n",
    "            \n",
    "         \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for index in pred:\n",
    "                    #if int(index)!=vocab_limit.index('eos'):\n",
    "                    if vocab_limit[int(index)] in string.punctuation or flag==0:\n",
    "                        print(str(vocab_limit[int(index)]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab_limit[int(index)]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for vec in train_summaries[i]:\n",
    "                    if vec2word(vec)!='eos':\n",
    "                        if vec2word(vec) in string.punctuation or flag==0:\n",
    "                            print(str(vec2word(vec)),end='')\n",
    "                        else:\n",
    "                            print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "                print(\"loss=\"+str(loss))\n",
    "            \n",
    "        step=step+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Try\\ To Do\\ To keep in mind:\n",
    "\n",
    "* Beam Search\n",
    "* Pointer Mechanisms\n",
    "* Heirarchical attention\n",
    "* [Intra-input-attention](https://arxiv.org/pdf/1705.04304.pdf)\n",
    "* Better pre-processing\n",
    "* Switch to PyTorch for dynamic models.\n",
    "* Mini-Batch Training\n",
    "* Better Datasets.\n",
    "* Train for different tasks (eg. Translation) using different datasets.\n",
    "* Intra-layer attention for both encoder and decoder together with everything else.\n",
    "* Adopt a more object oriented approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
